\documentclass[a4j,twocolumn]{jsarticle}
\usepackage{amsmath}
\usepackage{svg}
\usepackage{float}
\usepackage{svgcolor}
\usepackage{graphicx} % Required for inserting images
\usepackage{booktabs}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{mathtools}
\usepackage[subrefformat=parens]{subcaption}
% コード
\usepackage{listings,jvlisting}
% 背景色やテキスト色など、VSCodeに近い見た目に設定
\definecolor{commentGreen}{rgb}{0.12,0.49,0.14}
\definecolor{stringPurple}{rgb}{0.65, 0.12, 0.82}
\definecolor{keywordBlue}{rgb}{0.11,0.35,0.69}
\definecolor{basicBlack}{rgb}{0.0, 0.0, 0.0}
\definecolor{lineNumbers}{rgb}{0.5,0.5,0.5}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{white},
    basicstyle=\footnotesize\ttfamily\color{basicBlack},
    commentstyle=\color{commentGreen},
    keywordstyle=\color{keywordBlue},
    numberstyle=\tiny\color{lineNumbers},
    stringstyle=\color{stringPurple},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\renewcommand{\lstlistingname}{Code}

\title{scikit-learn、Keras、TensorFlowによる実践機械学習}
\author{東京大学 工学部 3年 阿部 慧人}
\date{2024年 8月 18日}

\begin{document}

\begin{abstract}
インターンの課題図書の4章から9章の演習問題を自分で解いたものの答えを載せる。
\end{abstract}

\maketitle
\section{4章 モデルの訓練}
\subsection{問題4.1}
数百万個もの特徴量を持つ訓練セットに対して、有効な線形回帰訓練アルゴリズムは、
バッチGD、確率的GD、ミニバッチGDである。これらの勾配降下法は特徴量の数$n$に対して、$O(n)$でしか計算量が
増加しない。一方で、正規方程式やSVDは、$O(n^{2})$以上の計算量がかかる。
\subsection{問題4.2}
特徴量のスケールが大きく異なる場合、勾配降下法を使うと時間がかかるというデメリットがある。
これは、損失関数の等高線が平たい形になり直線的に最小値に移動できないことが原因である。

\subsection{問題4.3}
ロジスティック回帰モデルの損失関数は、式(1)(2)である。
\begin{gather}
    J(\theta) = -\frac{1}{m}\Sigma[y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})] \\
    where  p = \sigma(\bm{x}^{T}\theta)
\end{gather}
この式から、$J(\theta)$は、$\theta$について凸関数とわかるので、極小値は唯一でそれを最小値とすればよい。

\subsection{問題4.4}
十分な時間を与えても前提とするモデルの枠組みによっては、同じモデルに収束するとは限らない。
具体的には、考えるモデルの損失関数の形による。極小値がただ一つであれば、その極小値に収束して、ただ一つの最適なモデルを得られる。
しかし、極小値が複数あると局所的な解に陥り、必ずしも最適な回にたどり着くとはいえない。

\subsection{問題4.5}
バッチ勾配降下法でエポックごとに検証誤差が大きくなっていると考えられるのならば、
過学習が生じていると考えることができる。このような時には、検証誤差が最小をとっているところで早期打ち切りをすることが望ましい。

\subsection{問題4.6}
ミニバッチ勾配降下法は、毎回の学習ステップで無作為に選んだインスタンスの集合を使って学習する。
そのため、その損失関数は確率的な変動を常に含む。よって、検証誤差が少し上昇したとしても、それは確率的な変動によるもの
である可能性もあるため、過学習だと決めて早期打ち切りをするのは不適切。

\subsection{問題4.7}
最も早く最適解近辺に辿り着くのは、確率的勾配降下法かバッチの小さいミニバッチ勾配降下法である。
ともに各イテレーションで行う演算が少ないため、短い時間で終わる。
一方で、収束はしない。これは、毎回確率的にインスタンスを選ぶことにより損失関数が上下するからである。
学習が進むにつれて、学習率を下げれば、収束する。

\subsection{問題4.8}
多項式回帰において、訓練誤差と検証誤差の間に大きな差があるというのは、過学習が生じていると考えられることはできる。
解決する手法としては、以下の三つが考えられる。1つが、モデルを正則化するということ。Ridge回帰やLasso回帰を使う。
2つ目が、多項式モデルの次数を下げること。次数が高いと、自由度が高く学習インスタンスに過剰に追従することがある。
3つ目が、学習インスタンスの数を増やすことである。

\subsection{問題4.9}
Ridge回帰を使っていて、訓練誤差と検証誤差がほとんと同じだが、非常に高い場合、バイアスが高いと考えられる。
正則化パラメータ$\alpha$を下げて、自由度を上げることで、バイアスを下げることができる。

\subsection{問題4.10}
線形回帰ではなく、Ridge回帰を使うべき理由は、線形回帰では自由度が高く過学習を引き起こすことが多いからである。

Ridge回帰ではなく、Lasso回帰を使うべき理由は、多くの場合意味のある特徴量が一部であり、Lasso回帰は意味のある特徴量の係数以外は
0にしてくれるので、意味のある特徴量のみ抽出できるから。

Lasso回帰ではなく、Elastic Netを使うべき理由は、訓練インスタンスの数よりも特徴量の数の方が多い時や特徴量間に相関がある時、
Lassoでは不規則な動きをすることがあるから。

\subsection{問題4.11}
2つのロジスティック回帰分類機を作るべき。屋外・屋内と日中・夜間というクラスは相互排他的ではない。
そのため、ソフトマックス回帰分類器のように多クラス出力できないものは役に立たない。

\subsection{問題4.12}
Jupyter Notebookに掲載。

\section{5章 サポートベクトルマシーン}
\subsection{問題5.1}
サポートベクトルマシーンの基本的な考え方は、二つのクラスの分類する時にその境界として「太さを持った道」を通すことである。
この道から最も近いインスタンスとの距離をマージンと呼ぶ。
このマージンをできるだけ多くすると、性能が良い分類器になる。その一方で、すべてのインスタンスを完全に分類する道を考えると、
マージンは小さくなる。この妥協点を見つけるのがポイント。

\subsection{問題5.2}
サポートベクトルとは、サポートベクトルマシーン(SVM)の境界となる「道」の境界と内側に存在するインスタンスのこと。
SVMの境界は、このサポートベクトルによってのみ決まり、それ以外のインスタンスは関係ない。

\subsection{問題5.3}
SVMを使うときに、入力をスケーリングするのはSVMは特徴量のスケールの影響を受けやすいからである。
スケーリングをした方が、決定境界の道が各特徴量を平等に考慮したものが出来上がる。

\subsection{問題5.4}
インスタンスの分類するときの確信度を数値化する方法として、決定協会からインスタンスまでの距離をスコアとしたものが考えられる。
しかし、これを直接確率に変換する方法はない。
scikit-learnでは、SVMクラスを使うときに、probability=Trueとすれば確率をロジスティック回帰によって計算するメソッドが追加される。

\subsection{問題5.5}
訓練インスタンスの数$m$に対して、計算量は主問題は$O(m)$で、双対問題は$O(m^{2})からO(m^{3})$であるから種問題を解く方がよい。

\subsection{問題5.6}
ガウスRBFカーネルを使ったSVMで過小適合しているときは、$\gamma$を大きくした方が良い。
$\gamma$を大きくすると、ベル曲線の幅が狭くなりより個別のインスタンスに適合するような形状になる。
また、ハイパーパラメータCについても、大きくするとマージン違反に厳しくなるため、よりインスタンスに適合する。

\subsection{問題5.7}
ソフトマージン線形SVM分類器をQPソルバーを使って解くことを考える。
このとき各変数は以下のように設定する。
\begin{equation}
\bm{H} = 
\begin{pmatrix} 
    0 & 0 & \dots  & 0 & \dots & 0 \\
    0 & 1 & \dots  & 0 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
    0 & 0 & \dots  & 1 & \dots & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots  & 0 & \dots & 0 \\
\end{pmatrix}
\end{equation}
\begin{equation}
\bm{f} = 
\begin{pmatrix} 
    0 &  \dots  & 0 & c & \dots & c \\
\end{pmatrix}
^\mathrm{T}
\end{equation}
\begin{equation}
    \bm{a'^{(i)}} = -t^{(i)} \times [1, \bm{x^{(i)}}]
\end{equation}
を満たす $m \times (n+1)$行列 $\bm{A'}$を考えた時、
\begin{equation}
    \bm{A} = 
\begin{pmatrix} 
    \bm{A'} &  I_{m}\\
    \bm{O} & -I_{m} \\
\end{pmatrix}
\end{equation}
\begin{equation}
    \bm{b} = 
    \begin{pmatrix} 
        -1 &  \dots  & -1 & 0 & \dots & 0 \\
    \end{pmatrix}
    ^\mathrm{T}
\end{equation}
このようにすることで、QPの条件式の前半のm個で、$w$に関する条件と後半のm個で$\zeta$に関する条件が得られる。

\subsection{問題5.8}
Jupyter Notebookに掲載。

\subsection{問題5.9}
Jupyter Notebookに掲載。

\subsection{問題5.10}
Jupyter Notebookに掲載。

\section{6章 決定木}
\subsection{問題6.1}
深さの制限を設けないような決定木を構成するとき、それはYesとNoを繰り返して二分木になることが考えられる。
よって、$log_{2}m \approx 18$となる。よって深さは、18ほどだと想定される。

\subsection{問題6.2}
一般に親ノードよりも子ノードの方がジニ不純度は小さくなる。
これは、CARTアルゴリズムでは子ノードのジニ不純度を小さくするように分割するようになっているから。
しかし、これは常に成り立つわけではない。というのも、CARTアルゴリズムは複数いる子ノードの不純度の加重平均
が小さくなるようにしているだけだから。インスタンスを多く含む子ノードを純粋にすることで、インスタンスの少ない別の子ノードの
不純度が親ノードのそれよりも上昇しても、結果としてその加重平均は親ノードよりも小さくなる。

\subsection{問題6.3}
過学習をしているときは、正則化ハイパーパラメータのよって条件をより厳しくすることが望まれる。
よって、max\_depthという上限は下げると良い。

\subsection{問題6.4}
入力特徴量の値を増やしても、スケーリングやセンタリングの影響を受けないから、意味はない。

\subsection{問題6.5}
計算量は、$O(n\times mlog_{2}m)$でインスタンスの数$mに対してmlog_{2}m$に比例する。
そのため、インスタンスの数が10倍になれば、$10log_{2}10m / log_{2}m \approx 12$時間になる。

\subsection{問題6.6}
インスタンスの数が数千未満のときは、presortすることで速度は上がるが、インスタンスの数が多いときは速度がかなり下がる。

\subsection{問題6.7}
Juyter Notebookに実施

\subsection{問題6.8}
Juyter Notebookに実施

\section{7章 アンサンブル学習とランダムフォレスト}
\subsection{問題7.1}
それぞれの異なるモデルの予測の結果を多数決するアンサンブルを構成すれば性能の向上が望める。
ここで大事なのは、それぞれのモデルが違うことである。もし可能であれば、それぞれのモデルが学習する訓練インスタンスも違うと、
より良い結果が望める。

\subsection{問題7.2}
ハード投票分類器は、単純にアンサンブルに使った異なる予測器の予測を集計して、その多数決を取りその結果をそのままアンサンブルの結果にする。
一方で、ソフト投票分類器はアンサンブルに使った異なる予測器のそれぞれのクラスに対する確率を集計して、
それぞれのクラスについての和をとって、その和が最大のものを答えとすること。

\subsection{問題7.3}
バギングアンサンブル、ペースティングアンサンブル、ランダムフォレストはそれぞれの予測器を独立に構成するため、
それぞれの学習を別のサーバーで並列処理しても問題なく、処理スピードは上昇することが考えられる。
ブ
ースティングアンサンブルは、Ada-boostingであれば前回の予測器の誤り率や勾配ブースティングであれば
前回の予測機の残差のように、前回の予測器の結果を使って次の予測器を構成する。
そのため、ブースティングアンサンブルは予測器間に依存関係があるため、並列処理はできない。

スタッキングアンサンブルでは、一つ前の層の予測機の結果を使って次の層の予測器を構成する。
そのため、同じ層の予測器は並列処理できるが、別の層は一つ前が終わるまで始められない。

\subsection{問題7.4}
OOB検証を使うと、検証セットをあらかじめ取り分けなくても、訓練セットのうち使われなかったもの、つまりOOB
を使ってバイアスが少ない形で検証できる。そのため全てを訓練セットに使うことができて性能が上がる。

\subsection{問題7.5}
Extra-Treesはランダムフォレストが使う特徴量をランダムに決めるのに加えて、その特徴量をどの閾値
で分けるかも無作為に決めるという点でさらにランダム性がある。
このようにすることで、バイアスは上がるが分散は下がるため、性能の向上を望める。
また、決定木は最適な閾値を見つけるのに時間がかかるため、その過程を省略することで時間がかなり早くなると考えられる。

\subsection{問題7.6}
アダブーストアンサンブルが過小適合指定いる場合は、推定器を増やすかそれぞれの推定器の正則化パラメータを下げる。
また、学習率を上げることも役にたつ。

\subsection{問題7.7}
勾配ブースティングアンサンブルが訓練セットに対して過学習を起こしているときは、個々の木の影響力を下げるため、学習率を下げると良い。
また、予測器の台数が多すぎて汎化性能が下がっている可能性があるため、早期打ち切りを実施するのも有効だろう。

\subsection{問題7.8}
Jupyter Notebookに掲載


\subsection{問題7.9}
Jupyter Notebookに掲載

\section{8章 次元削減}
\subsection{問題8.1}
データセットの次元を削減する主要な理由は、データセットの次元が高いとデータの密度がかなり低くなってしまい、
予測をする際に外挿を使う割合が増えて結果として、得られる予測器の信頼度が下がることを防ぐため。
また単純に次元を減らすことで時間計算量や空間計算量を減らすことができる。
さらに特徴量を減らすことで、重要な特徴量が何か見つけることができる。

一方で欠点として、次元削減によって重要な情報が落とされるとデータの信頼が下がる。
学習パイプラインが複雑化すること、また次元削減後のデータの解釈がしづらいことが問題としてあげられる。

\subsection{問題8.2}
「次元の呪い」とは次元が大きくなることで生じる様々な問題のことである。
その問題のうち主要なものは、高次元になればなるほどデータが疎になることである。すると、
過学習が生じやすいだけでなく、特徴量の中でパターンを見つけることが困難になり、学習の精度が下がる。

\subsection{問題8.3}
一般には完全に戻すことはできない。
例えば、PCAでも得られた主成分のうちd個のものを使うことで、d次元に落とし込み一部の情報が捨てられる。
そのため、逆変換行列を使って戻す時もその情報を再現することができない。

\subsection{問題8.4}
たとえ非線形なデータセットであったとしても、不要な次元を削除することができるからPCAは役に立つ。
しかし、全く不要な次元がないデータセット（例えばスイスロール）に対して、PCAを実施すると必要な情報が
失われるので、PCAは不適切ということになる。

\subsection{問題8.5}
因子寄与率の主成分ごとの分布はデータセットによってまちまちである。そのため、1000次元のデータであったら、
1から950次元のどの次元もとりいる。1次元になる場合は、ある1つの主成分に分散が集中しているようなデータセットであるし、
950次元になる場合は全ての主成分に等しい分散があるようなデータセットである。

\subsection{問題8.6}
通常のPCAは、特異値分解を使って全てのデータ群に対していっぺんに主成分を計算する。つまり、メモリが間に合わないと動作しない。
逐次学習型PCAは、データ数が多い時にデータをバッチごとに分割して実施するが、通常のPCAよりも動作速度は遅くなる。
ランダム化PCAは、次元を大きく削減したい時に使われる手法であり、厳密な特異値分解を実施しないため、大幅に早く実施できる。
カーネルPCAは、非線形なデータセットにも役に立つ。

\subsection{問題8.7}
データセットに対する次元削減アルゴリズムの性能評価は、主に2つ方法がある。
1つが、PCA自体は教師なし学習でも全体のプロセスが教師あり学習であることが多いため、最終的な予測の精度をもとに
グリッドサーチをすることができる。
2つ目が、完全に教師なしの場合は再構築誤差を使うという方法がある。次元削減の変換と逆変換を経て元のデータとどれくらいズレるかを確認することである。

\subsection{問題8.8}
次元削減アルゴリズムを連続して実施することで、動作速度を大幅の早くできるというメリットがある。
例えば、PCAを使ってざっくり次元削減をしてから、LLEを実施したとき、最初からLLEをやる場合と同じ精度の次元削減が望めるが、
実行時間は大幅に減少する。

\subsection{問題8.9}
Jupyter Notebookに実施。

ランダムフォレストアルゴリズムで、mnistデータの分類をした。
PCAの有無で動作速度と性能を比べたところ、
PCAをすることによって学習時間が、177.15秒から262.82秒に増えた。これは、次元削減をしたことによって逆に
決定境界が複雑になってしまったことを示唆する。
一方で、性能は0.9674から0.9487と微弱な低下に収まった。

ソフトマックス関数による分類器の場合、次元削減によって大幅に速度が上昇した。

\subsection{問題8.10}
Jupyter Notebookに実施。

各次元削減の手法の性能と動作時間を比較した。
その結果、PCAとt-SNEを組み合わせた手法が10000個の訓練データに対して154.6秒と最も速く、性能も良かった。
他の手法、単体のPCA、LLEやMDSは、うまくmnistのデータを分類できていなかった。


\section{9章 教師なし学習のテクニック}
\subsection{9.1}


%図の挿入
%   \centering
%    \includegraphics[keepaspectratio, width=0.9\columnwidth]{compile-link.pdf}
%   \caption{フリップフロップとラッチの波形}
%    \label{example}
%\end{figure}


\begin{thebibliography}{}
\bibitem{オライリー・ジャパン}
オライリー・ジャパン 「scikit-learn、Keras、TensorFlowによる実践機械学習」
\end{thebibliography}

\end{document}

