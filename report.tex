\documentclass[a4j,twocolumn]{jsarticle}
\usepackage{amsmath}
\usepackage{svg}
\usepackage{float}
\usepackage{svgcolor}
\usepackage{graphicx} % Required for inserting images
\usepackage{booktabs}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage[subrefformat=parens]{subcaption}
% コード
\usepackage{listings,jvlisting}
% 背景色やテキスト色など、VSCodeに近い見た目に設定
\definecolor{commentGreen}{rgb}{0.12,0.49,0.14}
\definecolor{stringPurple}{rgb}{0.65, 0.12, 0.82}
\definecolor{keywordBlue}{rgb}{0.11,0.35,0.69}
\definecolor{basicBlack}{rgb}{0.0, 0.0, 0.0}
\definecolor{lineNumbers}{rgb}{0.5,0.5,0.5}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{white},
    basicstyle=\footnotesize\ttfamily\color{basicBlack},
    commentstyle=\color{commentGreen},
    keywordstyle=\color{keywordBlue},
    numberstyle=\tiny\color{lineNumbers},
    stringstyle=\color{stringPurple},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\renewcommand{\lstlistingname}{Code}

\title{scikit-learn、Keras、TensorFlowによる実践機械学習}
\author{東京大学 工学部 3年 阿部 慧人}
\date{2024年 8月 18日}

\begin{document}

\begin{abstract}
インターンの課題図書の4章から9章の演習問題を自分で解いたものの答えを載せる。
\end{abstract}

\maketitle
\section{4章 モデルの訓練}
\subsection{問題4.1}
数百万個もの特徴量を持つ訓練セットに対して、有効な線形回帰訓練アルゴリズムは、
バッチGD、確率的GD、ミニバッチGDである。これらの勾配降下法は特徴量の数$n$に対して、$O(n)$でしか計算量が
増加しない。一方で、正規方程式やSVDは、$O(n^{2})$以上の計算量がかかる。
\subsection{問題4.2}
特徴量のスケールが大きく異なる場合、勾配降下法を使うと時間がかかるというデメリットがある。
これは、損失関数の等高線が平たい形になり直線的に最小値に移動できないことが原因である。

\subsection{問題4.3}
ロジスティック回帰モデルの損失関数は、式(1)(2)である。
\begin{gather}
    J(\theta) = -\frac{1}{m}\Sigma[y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})] \\
    where  p = \sigma(\bm{x}^{T}\theta)
\end{gather}
この式から、$J(\theta)$は、$\theta$について凸関数とわかるので、極小値は唯一でそれを最小値とすればよい。

\subsection{問題4.4}
十分な時間を与えても前提とするモデルの枠組みによっては、同じモデルに収束するとは限らない。
具体的には、考えるモデルの損失関数の形による。極小値がただ一つであれば、その極小値に収束して、ただ一つの最適なモデルを得られる。
しかし、極小値が複数あると局所的な解に陥り、必ずしも最適な回にたどり着くとはいえない。

\subsection{問題4.5}
バッチ勾配降下法でエポックごとに検証誤差が大きくなっていると考えられるのならば、
過学習が生じていると考えることができる。このような時には、検証誤差が最小をとっているところで早期打ち切りをすることが望ましい。

\subsection{問題4.6}
ミニバッチ勾配降下法は、毎回の学習ステップで無作為に選んだインスタンスの集合を使って学習する。
そのため、その損失関数は確率的な変動を常に含む。よって、検証誤差が少し上昇したとしても、それは確率的な変動によるもの
である可能性もあるため、過学習だと決めて早期打ち切りをするのは不適切。

\subsection{問題4.7}
最も早く最適解近辺に辿り着くのは、確率的勾配降下法かバッチの小さいミニバッチ勾配降下法である。
ともに各イテレーションで行う演算が少ないため、短い時間で終わる。
一方で、収束はしない。これは、毎回確率的にインスタンスを選ぶことにより損失関数が上下するからである。
学習が進むにつれて、学習率を下げれば、収束する。

\subsection{問題4.8}
多項式回帰において、訓練誤差と検証誤差の間に大きな差があるというのは、過学習が生じていると考えられることはできる。
解決する手法としては、以下の三つが考えられる。1つが、モデルを正則化するということ。Ridge回帰やLasso回帰を使う。
2つ目が、多項式モデルの次数を下げること。次数が高いと、自由度が高く学習インスタンスに過剰に追従することがある。
3つ目が、学習インスタンスの数を増やすことである。

\subsection{問題4.9}
Ridge回帰を使っていて、訓練誤差と検証誤差がほとんと同じだが、非常に高い場合、バイアスが高いと考えられる。
正則化パラメータ$\alpha$を下げて、自由度を上げることで、バイアスを下げることができる。

\subsection{問題4.10}
線形回帰ではなく、Ridge回帰を使うべき理由は、線形回帰では自由度が高く過学習を引き起こすことが多いからである。

Ridge回帰ではなく、Lasso回帰を使うべき理由は、多くの場合意味のある特徴量が一部であり、Lasso回帰は意味のある特徴量の係数以外は
0にしてくれるので、意味のある特徴量のみ抽出できるから。

Lasso回帰ではなく、Elastic Netを使うべき理由は、訓練インスタンスの数よりも特徴量の数の方が多い時や特徴量間に相関がある時、
Lassoでは不規則な動きをすることがあるから。

\subsection{問題4.11}
2つのロジスティック回帰分類機を作るべき。屋外・屋内と日中・夜間というクラスは相互排他的ではない。
そのため、ソフトマックス回帰分類器のように多クラス出力できないものは役に立たない。

\subsection{問題4.12}
Jupyter Notebookに掲載。

\section{5章 サポートベクトルマシーン}
\subsection{5.1}

\section{6章 決定木}
\subsection{6.1}

\section{7章 アンサンブル学習とランダムフォレスト}
\subsection{7.1}

\section{8章 次元削減}
\subsection{8.1}

\section{9章 教師なし学習のテクニック}
\subsection{9.1}


%図の挿入
%   \centering
%    \includegraphics[keepaspectratio, width=0.9\columnwidth]{compile-link.pdf}
%   \caption{フリップフロップとラッチの波形}
%    \label{example}
%\end{figure}


\begin{thebibliography}{}
\bibitem{オライリー・ジャパン}
オライリー・ジャパン 「scikit-learn、Keras、TensorFlowによる実践機械学習」
\end{thebibliography}

\end{document}

