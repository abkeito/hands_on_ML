\documentclass[a4j,twocolumn]{jsarticle}
\usepackage{amsmath}
\usepackage{svg}
\usepackage{float}
\usepackage{svgcolor}
\usepackage{graphicx} % Required for inserting images
\usepackage{booktabs}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{mathtools}
\usepackage[subrefformat=parens]{subcaption}
% コード
\usepackage{listings,jvlisting}
% 背景色やテキスト色など、VSCodeに近い見た目に設定
\definecolor{commentGreen}{rgb}{0.12,0.49,0.14}
\definecolor{stringPurple}{rgb}{0.65, 0.12, 0.82}
\definecolor{keywordBlue}{rgb}{0.11,0.35,0.69}
\definecolor{basicBlack}{rgb}{0.0, 0.0, 0.0}
\definecolor{lineNumbers}{rgb}{0.5,0.5,0.5}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{white},
    basicstyle=\footnotesize\ttfamily\color{basicBlack},
    commentstyle=\color{commentGreen},
    keywordstyle=\color{keywordBlue},
    numberstyle=\tiny\color{lineNumbers},
    stringstyle=\color{stringPurple},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\renewcommand{\lstlistingname}{Code}

\title{scikit-learn、Keras、TensorFlowによる実践機械学習}
\author{東京大学 工学部 3年 阿部 慧人}
\date{2024年 8月 18日}

\begin{document}

\begin{abstract}
インターンの課題図書の4章から9章の演習問題を自分で解いたものの答えを載せる。
\end{abstract}

\maketitle
\section{4章 モデルの訓練}
\subsection{問題4.1}
数百万個もの特徴量を持つ訓練セットに対して、有効な線形回帰訓練アルゴリズムは、
バッチGD、確率的GD、ミニバッチGDである。これらの勾配降下法は特徴量の数$n$に対して、$O(n)$でしか計算量が
増加しない。一方で、正規方程式やSVDは、$O(n^{2})$以上の計算量がかかる。
\subsection{問題4.2}
特徴量のスケールが大きく異なる場合、勾配降下法を使うと時間がかかるというデメリットがある。
これは、損失関数の等高線が平たい形になり直線的に最小値に移動できないことが原因である。

\subsection{問題4.3}
ロジスティック回帰モデルの損失関数は、式(1)(2)である。
\begin{gather}
    J(\theta) = -\frac{1}{m}\Sigma[y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})] \\
    where  p = \sigma(\bm{x}^{T}\theta)
\end{gather}
この式から、$J(\theta)$は、$\theta$について凸関数とわかるので、極小値は唯一でそれを最小値とすればよい。

\subsection{問題4.4}
十分な時間を与えても前提とするモデルの枠組みによっては、同じモデルに収束するとは限らない。
具体的には、考えるモデルの損失関数の形による。極小値がただ一つであれば、その極小値に収束して、ただ一つの最適なモデルを得られる。
しかし、極小値が複数あると局所的な解に陥り、必ずしも最適な回にたどり着くとはいえない。

\subsection{問題4.5}
バッチ勾配降下法でエポックごとに検証誤差が大きくなっていると考えられるのならば、
過学習が生じていると考えることができる。このような時には、検証誤差が最小をとっているところで早期打ち切りをすることが望ましい。

\subsection{問題4.6}
ミニバッチ勾配降下法は、毎回の学習ステップで無作為に選んだインスタンスの集合を使って学習する。
そのため、その損失関数は確率的な変動を常に含む。よって、検証誤差が少し上昇したとしても、それは確率的な変動によるもの
である可能性もあるため、過学習だと決めて早期打ち切りをするのは不適切。

\subsection{問題4.7}
最も早く最適解近辺に辿り着くのは、確率的勾配降下法かバッチの小さいミニバッチ勾配降下法である。
ともに各イテレーションで行う演算が少ないため、短い時間で終わる。
一方で、収束はしない。これは、毎回確率的にインスタンスを選ぶことにより損失関数が上下するからである。
学習が進むにつれて、学習率を下げれば、収束する。

\subsection{問題4.8}
多項式回帰において、訓練誤差と検証誤差の間に大きな差があるというのは、過学習が生じていると考えられることはできる。
解決する手法としては、以下の三つが考えられる。1つが、モデルを正則化するということ。Ridge回帰やLasso回帰を使う。
2つ目が、多項式モデルの次数を下げること。次数が高いと、自由度が高く学習インスタンスに過剰に追従することがある。
3つ目が、学習インスタンスの数を増やすことである。

\subsection{問題4.9}
Ridge回帰を使っていて、訓練誤差と検証誤差がほとんと同じだが、非常に高い場合、バイアスが高いと考えられる。
正則化パラメータ$\alpha$を下げて、自由度を上げることで、バイアスを下げることができる。

\subsection{問題4.10}
線形回帰ではなく、Ridge回帰を使うべき理由は、線形回帰では自由度が高く過学習を引き起こすことが多いからである。

Ridge回帰ではなく、Lasso回帰を使うべき理由は、多くの場合意味のある特徴量が一部であり、Lasso回帰は意味のある特徴量の係数以外は
0にしてくれるので、意味のある特徴量のみ抽出できるから。

Lasso回帰ではなく、Elastic Netを使うべき理由は、訓練インスタンスの数よりも特徴量の数の方が多い時や特徴量間に相関がある時、
Lassoでは不規則な動きをすることがあるから。

\subsection{問題4.11}
2つのロジスティック回帰分類機を作るべき。屋外・屋内と日中・夜間というクラスは相互排他的ではない。
そのため、ソフトマックス回帰分類器のように多クラス出力できないものは役に立たない。

\subsection{問題4.12}
Jupyter Notebookに掲載。

\section{5章 サポートベクトルマシーン}
\subsection{問題5.1}
サポートベクトルマシーンの基本的な考え方は、二つのクラスの分類する時にその境界として「太さを持った道」を通すことである。
この道から最も近いインスタンスとの距離をマージンと呼ぶ。
このマージンをできるだけ多くすると、性能が良い分類器になる。その一方で、すべてのインスタンスを完全に分類する道を考えると、
マージンは小さくなる。この妥協点を見つけるのがポイント。

\subsection{問題5.2}
サポートベクトルとは、サポートベクトルマシーン(SVM)の境界となる「道」の境界と内側に存在するインスタンスのこと。
SVMの境界は、このサポートベクトルによってのみ決まり、それ以外のインスタンスは関係ない。

\subsection{問題5.3}
SVMを使うときに、入力をスケーリングするのはSVMは特徴量のスケールの影響を受けやすいからである。
スケーリングをした方が、決定境界の道が各特徴量を平等に考慮したものが出来上がる。

\subsection{問題5.4}
インスタンスの分類するときの確信度を数値化する方法として、決定協会からインスタンスまでの距離をスコアとしたものが考えられる。
しかし、これを直接確率に変換する方法はない。
scikit-learnでは、SVMクラスを使うときに、probability=Trueとすれば確率をロジスティック回帰によって計算するメソッドが追加される。

\subsection{問題5.5}
訓練インスタンスの数$m$に対して、計算量は主問題は$O(m)$で、双対問題は$O(m^{2})からO(m^{3})$であるから種問題を解く方がよい。

\subsection{問題5.6}
ガウスRBFカーネルを使ったSVMで過小適合しているときは、$\gamma$を大きくした方が良い。
$\gamma$を大きくすると、ベル曲線の幅が狭くなりより個別のインスタンスに適合するような形状になる。
また、ハイパーパラメータCについても、大きくするとマージン違反に厳しくなるため、よりインスタンスに適合する。

\subsection{問題5.7}
ソフトマージン線形SVM分類器をQPソルバーを使って解くことを考える。
このとき各変数は以下のように設定する。
\begin{equation}
\bm{H} = 
\begin{pmatrix} 
    0 & 0 & \dots  & 0 & \dots & 0 \\
    0 & 1 & \dots  & 0 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
    0 & 0 & \dots  & 1 & \dots & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots  & 0 & \dots & 0 \\
\end{pmatrix}
\end{equation}
\begin{equation}
\bm{f} = 
\begin{pmatrix} 
    0 &  \dots  & 0 & c & \dots & c \\
\end{pmatrix}
^\mathrm{T}
\end{equation}
\begin{equation}
    \bm{a'^{(i)}} = -t^{(i)} \times [1, \bm{x^{(i)}}]
\end{equation}
を満たす $m \times (n+1)$行列 $\bm{A'}$を考えた時、
\begin{equation}
    \bm{A} = 
\begin{pmatrix} 
    \bm{A'} &  I_{m}\\
    \bm{O} & -I_{m} \\
\end{pmatrix}
\end{equation}
\begin{equation}
    \bm{b} = 
    \begin{pmatrix} 
        -1 &  \dots  & -1 & 0 & \dots & 0 \\
    \end{pmatrix}
    ^\mathrm{T}
\end{equation}
このようにすることで、QPの条件式の前半のm個で、$w$に関する条件と後半のm個で$\zeta$に関する条件が得られる。

\subsection{問題5.8}
Jupyter Notebookに掲載。

\subsection{問題5.9}
Jupyter Notebookに掲載。

\subsection{問題5.10}
Jupyter Notebookに掲載。

\section{6章 決定木}
\subsection{問題6.1}
深さの制限を設けないような決定木を構成するとき、それはYesとNoを繰り返して二分木になることが考えられる。
よって、$log_{2}m \approx 18$となる。よって深さは、18ほどだと想定される。

\subsection{問題6.2}
一般に親ノードよりも子ノードの方がジニ不純度は小さくなる。
これは、CARTアルゴリズムでは子ノードのジニ不純度を小さくするように分割するようになっているから。
しかし、これは常に成り立つわけではない。というのも、CARTアルゴリズムは複数いる子ノードの不純度の加重平均
が小さくなるようにしているだけだから。インスタンスを多く含む子ノードを純粋にすることで、インスタンスの少ない別の子ノードの
不純度が親ノードのそれよりも上昇しても、結果としてその加重平均は親ノードよりも小さくなる。

\subsection{問題6.3}
過学習をしているときは、正則化ハイパーパラメータのよって条件をより厳しくすることが望まれる。
よって、max\_depthという上限は下げると良い。

\subsection{問題6.4}
入力特徴量の値を増やしても、スケーリングやセンタリングの影響を受けないから、意味はない。

\subsection{問題6.5}
計算量は、$O(n\times mlog_{2}m)$でインスタンスの数$mに対してmlog_{2}m$に比例する。
そのため、インスタンスの数が10倍になれば、$10log_{2}10m / log_{2}m \approx 12$時間になる。

\subsection{問題6.6}
インスタンスの数が数千未満のときは、presortすることで速度は上がるが、インスタンスの数が多いときは速度がかなり下がる。

\subsection{問題6.7}
Juyter Notebookに実施

\subsection{問題6.8}
Juyter Notebookに実施

\section{7章 アンサンブル学習とランダムフォレスト}
\subsection{7.1}

\section{8章 次元削減}
\subsection{8.1}

\section{9章 教師なし学習のテクニック}
\subsection{9.1}


%図の挿入
%   \centering
%    \includegraphics[keepaspectratio, width=0.9\columnwidth]{compile-link.pdf}
%   \caption{フリップフロップとラッチの波形}
%    \label{example}
%\end{figure}


\begin{thebibliography}{}
\bibitem{オライリー・ジャパン}
オライリー・ジャパン 「scikit-learn、Keras、TensorFlowによる実践機械学習」
\end{thebibliography}

\end{document}

